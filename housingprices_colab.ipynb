{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4mwmPUP98Oe",
        "outputId": "74f8a008-f90d-4bb1-9797-f8a175be50f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Connected to cloud.r-project.org (52.85.151.93)] [Co\r                                                                                                    \rGet:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "\r                                                                                                    \rGet:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "\r                                                                                                    \rGet:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\r0% [2 InRelease 50.4 kB/119 kB 42%] [3 InRelease 101 kB/110 kB 92%] [Connecting to ppa.launchpadcont\r0% [2 InRelease 64.9 kB/119 kB 55%] [Connecting to ppa.launchpadcontent.net (185.125.190.80)] [Waiti\r0% [Waiting for headers] [Connecting to ppa.launchpadcontent.net (185.125.190.80)] [Waiting for head\r                                                                                                    \rHit:5 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:9 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,624 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [2,048 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,080 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,903 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,356 kB]\n",
            "Get:16 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [783 kB]\n",
            "Fetched 9,028 kB in 2s (5,344 kB/s)\n",
            "Reading package lists... Done\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import joblib\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "import os\n",
        "# get latest version of spark\n",
        "spark_version = 'spark-3.5.1'\n",
        "os.environ['SPARK_VERSION']=spark_version\n",
        "\n",
        "# Install Spark and Java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop3.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set Environment Variables\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop3\"\n",
        "\n",
        "# Start a SparkSession\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTFqpFYP-ghS"
      },
      "outputs": [],
      "source": [
        "# Import packages\n",
        "from pyspark.sql import SparkSession\n",
        "import time\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder\\\n",
        "    .appName(\"SparkSQL\")\\\n",
        "    .config(\"spark.sql.debug.maxToStringFields\", 2000)\\\n",
        "    .config(\"spark.driver.memory\", \"2g\")\\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Set the partitions to 4 or 8.\n",
        "spark.conf.set(\"spark.sql.shuffle.partitions\", 8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jKFd8L_-ln0"
      },
      "outputs": [],
      "source": [
        "# Read in housing data from S3 Bucket\n",
        "from pyspark import SparkFiles\n",
        "url = \"https://oleslamburgerbucket.s3.us-west-2.amazonaws.com/realtor-data.zip.csv\"\n",
        "spark.sparkContext.addFile(url)\n",
        "us_housing_df = spark.read.csv(SparkFiles.get(\"realtor-data.zip.csv\"), sep=\",\", header=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hbVvubu-MCxV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhHl3Z-U-xzA"
      },
      "outputs": [],
      "source": [
        "# examine the dataset\n",
        "us_housing_df.show(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgGSG_vgBDuD"
      },
      "outputs": [],
      "source": [
        "#get a list of data types for columns\n",
        "us_housing_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhJxJ_Z6BYxt"
      },
      "outputs": [],
      "source": [
        "# Convert numeric values to floats, integers, and dates\n",
        "us_housing_df = us_housing_df.withColumn('bed',us_housing_df['bed'].cast('float'))\n",
        "us_housing_df = us_housing_df.withColumn('bath',us_housing_df['bath'].cast('Int'))\n",
        "us_housing_df = us_housing_df.withColumn('acre_lot',us_housing_df['acre_lot'].cast('float'))\n",
        "us_housing_df = us_housing_df.withColumn('zip_code',us_housing_df['zip_code'].cast('Int'))\n",
        "us_housing_df = us_housing_df.withColumn('house_size',us_housing_df['house_size'].cast('float'))\n",
        "us_housing_df = us_housing_df.withColumn('prev_sold_date',us_housing_df['prev_sold_date'].cast('date'))\n",
        "us_housing_df = us_housing_df.withColumn('price',us_housing_df['price'].cast('float'))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9oWwD7ae523"
      },
      "outputs": [],
      "source": [
        "us_housing_df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7xPKoXl4Kik"
      },
      "outputs": [],
      "source": [
        "df = us_housing_df.toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "HyjDwrpnk3Bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "6-g2p1ZLpgEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "sZQzNqPxpgGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualizing the missing values by column\n",
        "sns.barplot(df.isnull().sum().sort_values(ascending=False), palette='bright')\n",
        "plt.ylim(0, 70000)\n",
        "plt.xticks(rotation = 90);"
      ],
      "metadata": {
        "id": "rYWPQPSApgJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Cleaning"
      ],
      "metadata": {
        "id": "t7533qbsQVmb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# drop the prev_sold_date column because it is missing half the values\n",
        "df = df.drop('prev_sold_date', axis = 1)"
      ],
      "metadata": {
        "id": "mynK0cMUpgL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop null values for city, zip_code and price\n",
        "df.dropna(subset=['city', 'zip_code', 'price'], inplace=True)"
      ],
      "metadata": {
        "id": "w8bQ0nsvpgOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop all rows where bed and bath values are missing\n",
        "df = df.drop(df[(df['bed'].isnull()) & (df['bath'].isnull())].index, axis = 0)"
      ],
      "metadata": {
        "id": "KOjZH6rfpgQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop all rows where acre_lot and house_size values are missing\n",
        "df = df.drop(df[(df['acre_lot'].isnull()) & (df['house_size'].isnull())].index, axis = 0)"
      ],
      "metadata": {
        "id": "1YDgtS7SpgTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop all rows where bath and house_size values are missing\n",
        "df = df.drop(df[(df['bed'].isnull()) & (df['house_size'].isnull())].index, axis = 0)"
      ],
      "metadata": {
        "id": "dYoNXbIepgXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop all rows where bed and house_size values are missing\n",
        "df = df.drop(df[(df['bath'].isnull()) & (df['house_size'].isnull())].index, axis = 0)"
      ],
      "metadata": {
        "id": "Y0dSQFD-pgZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use linear regression to predict missing values for house_size\n",
        "# the features we will use are bed, bath, acre_lot and price\n",
        "df.corr()['house_size'].sort_values(ascending=False)[1:]"
      ],
      "metadata": {
        "id": "7B4l88dVpgcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gather training data from df and drop nulls\n",
        "house_size_df = df[['bed', 'bath', 'acre_lot', 'price', 'house_size']].dropna()"
      ],
      "metadata": {
        "id": "skbKoJIhpgfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c9lJ66NcDgF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check for outliers that may skew results\n",
        "sns.boxplot(house_size_df['house_size'].sort_values(ascending=False))\n",
        "house_size_df['house_size'].sort_values(ascending=False)[:10]"
      ],
      "metadata": {
        "id": "dOme2WPbpghe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop rows with house_size values over 5000\n",
        "house_size_df = house_size_df[(house_size_df['house_size'] < 5000) & (house_size_df['house_size'] > 400)]\n",
        "\n",
        "sns.boxplot(house_size_df['house_size'].sort_values(ascending=False))"
      ],
      "metadata": {
        "id": "Zc1o0UFEpgj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(house_size_df)"
      ],
      "metadata": {
        "id": "M8Z_kVq9pgmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "house_size_df"
      ],
      "metadata": {
        "id": "Yg2IUpSbF0kM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# separate features and target\n",
        "# split the data\n",
        "X = house_size_df.drop('house_size', axis = 1)\n",
        "y = house_size_df['house_size']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "Q-2IHLVopgou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scale the data\n",
        "house_scaler = StandardScaler()\n",
        "X_train_scaled = house_scaler.fit_transform(X_train)\n",
        "X_test_scaled = house_scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "r_E8onQ0pgrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create and train the model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train_scaled,y_train)"
      ],
      "metadata": {
        "id": "DMji8NJVpgtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get predictions and evaluate\n",
        "y_hat = model.predict(X_test_scaled)\n",
        "print(f'mae: {mean_absolute_error(y_test, y_hat)}')\n",
        "print(f'mse: {mean_squared_error(y_test, y_hat)}')\n",
        "print(f'mse: {np.sqrt(mean_squared_error(y_test, y_hat))}')"
      ],
      "metadata": {
        "id": "aBkWt11dpgwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mean absolute error is 20% of the average house size, not the best accuracy but also not too bad.\n",
        "420/house_size_df['house_size'].mean()"
      ],
      "metadata": {
        "id": "ZIQzHhWWpgyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use a sequential model to make predictions instead\n",
        "tf_model = Sequential()\n",
        "tf_model.add(Dense(28, activation = 'relu'))\n",
        "tf_model.add(Dense(14, activation = 'relu'))\n",
        "tf_model.add(Dense(7, activation = 'relu'))\n",
        "tf_model.add(Dense(1))\n",
        "\n",
        "tf_model.compile(loss='mean_absolute_error', optimizer = 'adam', metrics = ['mae'])"
      ],
      "metadata": {
        "id": "7NQtX-btpg05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop = EarlyStopping(patience = 1, monitor = 'val_loss')\n",
        "tf_model.fit(X_train_scaled, y_train, epochs = 100, validation_data=(X_test_scaled, y_test), callbacks = [stop])"
      ],
      "metadata": {
        "id": "49BK1c5ypg3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sequential model performed a little bit better\n",
        "# gather the rows and features where house_size is null and store in df\n",
        "X_house_size = df[df['house_size'].isnull()][['bed', 'bath', 'acre_lot', 'price']]"
      ],
      "metadata": {
        "id": "Oje8M7T1pg5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scale the data\n",
        "X_house_size_scaled = house_scaler.fit_transform(X_house_size)"
      ],
      "metadata": {
        "id": "q7SKYpZgpg7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use the sequential model to predict\n",
        "house_size_preds = tf_model.predict(X_house_size_scaled)"
      ],
      "metadata": {
        "id": "-n95ix6Spg9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# grab indices of rows where house_size is missing data\n",
        "house_size_null_index = df[df['house_size'].isnull()].index\n",
        "\n",
        "# reshape predictions to be 1 dimension and round the numbers off\n",
        "# put into a series and set the index to match the index of our df of missing house_size values\n",
        "house_size_preds_series = pd.Series(np.round(house_size_preds.reshape(-1))).set_axis(house_size_null_index)\n",
        "\n",
        "# fill the values in place and update the house_size column\n",
        "df['house_size'] = df['house_size'].fillna(house_size_preds_series)"
      ],
      "metadata": {
        "id": "l9iY1sHhpg_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "aUybqby_k3gI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now do the same for the acre_lot column using the house_size, price and state 'house_size', 'state', 'price', 'acre_lot'\n",
        "acre_lot_df = df[['house_size', 'state', 'price', 'acre_lot']].dropna()"
      ],
      "metadata": {
        "id": "xLQ2Xfbsq8-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check for outliers that may skew results\n",
        "sns.boxplot(acre_lot_df['acre_lot'].sort_values(ascending=False))\n",
        "house_size_df['acre_lot'].sort_values(ascending=False)[:10]"
      ],
      "metadata": {
        "id": "l-KC-S6Dq9FE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop rows with acre_lot values over 10 acres and less than 0.01\n",
        "acre_lot_df = acre_lot_df[(acre_lot_df['acre_lot'] <= 10) & (acre_lot_df['acre_lot'] > 0.01)]\n",
        "sns.boxplot(acre_lot_df['acre_lot'].sort_values(ascending=False))"
      ],
      "metadata": {
        "id": "STnF3n9tq9H4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get dummies for the state column\n",
        "acre_lot_df = pd.get_dummies(acre_lot_df)"
      ],
      "metadata": {
        "id": "ag25P8aKq9K5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# separate features and target\n",
        "# split the data\n",
        "X = acre_lot_df.drop('acre_lot', axis = 1)\n",
        "y = acre_lot_df['acre_lot']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "fwz8siQwq9Nf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scale the data\n",
        "acre_scaler = StandardScaler()\n",
        "X_train_scaled = acre_scaler.fit_transform(X_train)\n",
        "X_test_scaled = acre_scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "JjAHfS6-q9Qe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create and train the model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train_scaled,y_train)"
      ],
      "metadata": {
        "id": "Y9d_fHDZq9WN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get predictions and evaluate\n",
        "y_hat = model.predict(X_test_scaled)\n",
        "print(f'mae: {mean_absolute_error(y_test, y_hat)}')\n",
        "print(f'mse: {mean_squared_error(y_test, y_hat)}')\n",
        "print(f'mse: {np.sqrt(mean_squared_error(y_test, y_hat))}')"
      ],
      "metadata": {
        "id": "L2YcnCuXq9ZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mae is even higher than the average acre_lot size so this model is very inaccurate with the available features.\n",
        "acre_lot_df['acre_lot'].mean()"
      ],
      "metadata": {
        "id": "ubDO0xd_q9b3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# few of the properties have over 50 acres so we will drop anything over 50 acres\n",
        "# then fill null values with the mean instead\n",
        "df = df[df['acre_lot'] <= 50]"
      ],
      "metadata": {
        "id": "3uRa2qk9q9e-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['acre_lot'].fillna(df['acre_lot'].mean(), inplace=True)"
      ],
      "metadata": {
        "id": "QNmetBXwq9hg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "5qeR9z-iq9kR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop remaining missing values\n",
        "df = df.dropna(subset=['bed', 'bath'])"
      ],
      "metadata": {
        "id": "mKjYDQmNq9m0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "8JhyQjwaq9ps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clean up more outliers in the dataset\n",
        "df = df[(df['house_size'] < 10000) & (df['house_size'] > 400)]\n",
        "df = df[(df['acre_lot'] < 10) & (df['acre_lot'] > 0.01)]\n",
        "df = df[df['bed'] <= 10]\n",
        "df = df[df['bath'] <= 10]"
      ],
      "metadata": {
        "id": "rE9DI7lKq9sf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(df['price'])"
      ],
      "metadata": {
        "id": "2fUUYjdeq9vX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['price'].sort_values(ascending=False)[:10]"
      ],
      "metadata": {
        "id": "hqw_hXK_q9ye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df[df['price'] > 10000000])"
      ],
      "metadata": {
        "id": "69rTYC6sk3l6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove price values over 10 million and under 25000\n",
        "df = df[(df['price'] < 10000000) & (df['price'] > 25000)]"
      ],
      "metadata": {
        "id": "-4gEx2Yok3pG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reset the index\n",
        "df.reset_index(inplace=True, drop = True)"
      ],
      "metadata": {
        "id": "Bh2iwu47k3sT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "bsB_CfcRk3vg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('cleaned_realtor_data.csv', index = False)"
      ],
      "metadata": {
        "id": "2ReOV_yUr8dv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizations"
      ],
      "metadata": {
        "id": "4ON5fM6FPB6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.corr()['price'].sort_values(ascending=False)[1:]"
      ],
      "metadata": {
        "id": "SMxMc_xdPA8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create heatmap of correlations\n",
        "sns.heatmap(df.corr(), annot = True, cmap='viridis')"
      ],
      "metadata": {
        "id": "z2QOD95GPZB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scatterplot showing correlation between house_size and price\n",
        "sns.scatterplot(data=df.head(100000), x='house_size', y='price', alpha = .5)"
      ],
      "metadata": {
        "id": "f1OdkERzPbLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clear correlation between bath and price using a boxplot\n",
        "sns.boxplot(data=df, x='bath', y = 'price', palette = 'bright', hue = 'bath')\n",
        "plt.legend(loc = (1.1,0))"
      ],
      "metadata": {
        "id": "dXyETloTPoRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(data=df, x='bed', y = 'price', palette = 'bright', hue = 'bed')\n",
        "plt.legend(loc = (1.1,0))"
      ],
      "metadata": {
        "id": "dOyN78HWPoZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.histplot(data=df, x = 'price', bins = 30)\n",
        "plt.ylim(0,50000)"
      ],
      "metadata": {
        "id": "bV-dIy6PPol6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare the data for making price predictions with Scikit-Learn and Tensorflow"
      ],
      "metadata": {
        "id": "2x8uSZchP2mu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# drop the for_sale column because there is only 1 unique value\n",
        "df['status'].unique()\n",
        "df = df.drop('status', axis = 1 )"
      ],
      "metadata": {
        "id": "TF3_QZC6r8nT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['city'].nunique()"
      ],
      "metadata": {
        "id": "lbxhd1k8r8qJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cities has nearly 5000 unique values so we will convert the cities into their respective value counts/frequencies\n",
        "city_counts = df['city'].value_counts()\n",
        "df['city'] = df['city'].apply(lambda x: city_counts.loc[x])"
      ],
      "metadata": {
        "id": "WFZCAUEyr8tY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['zip_code'].nunique()"
      ],
      "metadata": {
        "id": "2sNPgMPjr84l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we will do the same with zip_code to reduce the size of the values\n",
        "zip_code_counts = df['zip_code'].value_counts()\n",
        "df['zip_code'] = df['zip_code'].apply(lambda x: zip_code_counts.loc[x])"
      ],
      "metadata": {
        "id": "An4QI6MYr88J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use the OneHotEncoder to get dummies for the state column\n",
        "ohe = OneHotEncoder(handle_unknown='ignore', sparse_output= False).set_output(transform= 'pandas')"
      ],
      "metadata": {
        "id": "FooVCKwSr9AX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ohetransform = ohe.fit_transform(df[['state']])"
      ],
      "metadata": {
        "id": "GuFc0jszr9EZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.concat([df, ohetransform], axis = 1).drop(['state'], axis = 1)"
      ],
      "metadata": {
        "id": "-onw8_ftr9Jm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "30pApj2ir9O_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split the data into features and the target value\n",
        "X = df.drop('price', axis = 1)\n",
        "y = df['price']"
      ],
      "metadata": {
        "id": "O0-at-8Br9Vb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=.20, random_state=42)"
      ],
      "metadata": {
        "id": "8Q6QHMX8r9ZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "iR3jU2pur9cu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(scaler, 'scaler.pkl')"
      ],
      "metadata": {
        "id": "jnBIFKMur9hC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Begin Training The Model"
      ],
      "metadata": {
        "id": "GwuuDy7lQyJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create linear regression model\n",
        "# fit to training data\n",
        "linear_model = LinearRegression()\n",
        "linear_model.fit(X_train_scaled, y_train)"
      ],
      "metadata": {
        "id": "QSwC4UVLr9om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make predictions\n",
        "lr_preds = linear_model.predict(X_test_scaled)"
      ],
      "metadata": {
        "id": "sKawMPtwsiGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use metrics to analyze results\n",
        "print(f'mae: {mean_absolute_error(y_test, lr_preds)}')\n",
        "print(f'mse: {mean_squared_error(y_test, lr_preds)}')\n",
        "print(f'rmse: {np.sqrt(mean_squared_error(y_test, lr_preds))}')"
      ],
      "metadata": {
        "id": "vnyKuzr4siIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the linear regression model does not produce desireable results\n",
        "# we will now try RandomForestRegressor\n",
        "rf_model = RandomForestRegressor(n_estimators = 130)"
      ],
      "metadata": {
        "id": "wTk-YtlTsiLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_model.fit(X_train_scaled, y_train)"
      ],
      "metadata": {
        "id": "sc5-cuEFsiOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_preds = rf_model.predict(X_test_scaled)\n",
        "print(f'mae: {mean_absolute_error(y_test, rf_preds)}')\n",
        "print(f'mse: {mean_squared_error(y_test, rf_preds)}')\n",
        "print(f'rmse: {np.sqrt(mean_squared_error(y_test, rf_preds))}')"
      ],
      "metadata": {
        "id": "ZFYNXlszsiR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RandomForestRegressor is nearly perfect at predicting housing prices\n",
        "sns.kdeplot(data=rf_preds, label = 'predictions')\n",
        "sns.kdeplot(data=y_test, label = 'actual')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "nsfWRPUAsiVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(rf_model, \"housing-price-model.pkl\")"
      ],
      "metadata": {
        "id": "PTxIrQsWsiYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our RandomForestRegressor Has Nearly Perfect Accuracy"
      ],
      "metadata": {
        "id": "k8EYCp3uR4Rl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "city_series = df['city'].value_counts()"
      ],
      "metadata": {
        "id": "8ac7OWBxsibn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zip_series = df['zip_code'].value_counts()"
      ],
      "metadata": {
        "id": "SPbBip21siei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save series to csv for preprocessing (converting data to their value counts) with new data\n",
        "city_series.to_csv('city_series.csv')\n",
        "zip_series.to_csv('zip_series.csv')"
      ],
      "metadata": {
        "id": "KZkK_AXxsihN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get dataframe where each row has a unique state\n",
        "# this df will be used to create a small dataframe to allow for one hot encoding when new data is added\n",
        "unique_states_df = df.drop_duplicates('state')\n",
        "unique_states_df = unique_states_df.drop(['price', 'status'], axis = 1)\n",
        "unique_states_df.head()"
      ],
      "metadata": {
        "id": "TOc9GB1OsikL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_states_df.to_csv('unique_states_df.csv', index = False)"
      ],
      "metadata": {
        "id": "gL12nuCAsim6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load in necessary files to run custom function below\n",
        "zip_series = pd.read_csv('zip_series.csv')\n",
        "city_series = pd.read_csv('city_series.csv')\n",
        "loaded_scaler = joblib.load('housing_scaler.pkl')\n",
        "loaded_model = joblib.load('housing_model.pkl')"
      ],
      "metadata": {
        "id": "slwkAAScsiqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a function to automatically preprocess new data\n",
        "def preprocessing(unique_states_df, user_row, city_series, zip_series):\n",
        "\n",
        "    # convert state and city to title case\n",
        "    user_row['city'] = user_row['city'].str.title()\n",
        "    user_row['state'] = user_row['state'].str.title()\n",
        "\n",
        "    # read in city and zip csv files\n",
        "    city_series = city_series.copy(deep=True)\n",
        "    zip_series = zip_series.copy(deep=True)\n",
        "\n",
        "    # convert to a series\n",
        "    city_series.set_index('Unnamed: 0', inplace=True)\n",
        "    city_series = city_series['city']\n",
        "\n",
        "    try:\n",
        "        # convert to city count\n",
        "        user_row['city'] = city_series.loc[user_row['city'].iloc[0]]\n",
        "\n",
        "    except:\n",
        "        # if city not in training data, replace with 1\n",
        "        city = user_row['city'].iloc[0]\n",
        "        print(f'Model has never seen \"{city}\" before. Estimate may be inaccurate.')\n",
        "        user_row['city'] = 1\n",
        "\n",
        "    # convert to a series\n",
        "    zip_series.set_index('Unnamed: 0', inplace=True)\n",
        "    zip_series = zip_series['zip_code']\n",
        "\n",
        "    try:\n",
        "        # convert to zip count\n",
        "        user_row['zip_code'] = zip_series.loc[user_row['zip_code'].iloc[0]]\n",
        "\n",
        "    except:\n",
        "        # if zip not in training data, replace with 1\n",
        "        zip = user_row['zip_code'].iloc[0]\n",
        "        print(f'Model has never seen zipcode \"{zip}\" before. Estimate may be inaccurate.')\n",
        "        user_row['zip_code'] = 1\n",
        "\n",
        "    unique_states_df['city'] = unique_states_df['city'].apply(lambda x: int(x == 0))\n",
        "\n",
        "    # add row to unique df\n",
        "    new_df = pd.concat([unique_states_df, user_row])\n",
        "\n",
        "    # create encoder\n",
        "    ohe = OneHotEncoder(handle_unknown='ignore', sparse_output= False).set_output(transform= 'pandas')\n",
        "    ohetransform = ohe.fit_transform(new_df[['state']])\n",
        "    new_df = pd.concat([new_df, ohetransform], axis=1).drop(['state'], axis=1)\n",
        "\n",
        "    new_df = loaded_scaler.transform(new_df)\n",
        "\n",
        "    return [new_df[-1]]\n"
      ],
      "metadata": {
        "id": "RHbqJf40SFtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run a test on data it hasn't seen before\n",
        "target_value = df.loc[[X_test.index[0]]]['price'].iloc[0]\n",
        "print(f'price target: ${target_value}')\n",
        "test_row = df.loc[[X_test.index[0]]]\n",
        "test_row = test_row.drop(['status', 'price'], axis = 1)\n",
        "test_row"
      ],
      "metadata": {
        "id": "wGW2tkrNSGAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = preprocessing(unique_states_df, test_row, city_series, zip_series)"
      ],
      "metadata": {
        "id": "75Lhf3QLSGD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model.predict(test)[0]"
      ],
      "metadata": {
        "id": "KEzPaFgTSGIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The RandomForestRegressor Was 100% Accurate With New Data That Was Preprocessed Using Our Function"
      ],
      "metadata": {
        "id": "bx8zreeMSc1P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Making Price Predictions with Tensorflow"
      ],
      "metadata": {
        "id": "IhOXlgSwSjJX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf_price_model = Sequential()\n",
        "\n",
        "tf_price_model.add(Dense(256, activation = 'relu', input_shape = (25,)))\n",
        "tf_price_model.add(Dense(256, activation = 'relu'))\n",
        "tf_price_model.add(Dense(128, activation = 'relu'))\n",
        "tf_price_model.add(Dense(64, activation = 'relu'))\n",
        "tf_price_model.add(Dense(1))\n",
        "\n",
        "tf_price_model.compile(loss='mean_absolute_error', optimizer = 'adam', metrics = ['mae'])\n",
        "tf_price_model.summary()"
      ],
      "metadata": {
        "id": "NdtGcon1Sdzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop = EarlyStopping(patience = 3, monitor = 'val_loss')\n",
        "trained_tf_model = tf_price_model.fit(X_train_scaled, y_train, epochs = 300, validation_data=(X_test_scaled, y_test), callbacks=[stop])"
      ],
      "metadata": {
        "id": "n-JolTRySnnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = pd.DataFrame(tf_price_model.history.history)"
      ],
      "metadata": {
        "id": "hoewBIOGSnwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history.plot()"
      ],
      "metadata": {
        "id": "Vn7u7CUpSn0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_model_preds = tf_price_model.predict(X_test_scaled)"
      ],
      "metadata": {
        "id": "Roi3oHSSSn3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print metrics\n",
        "print(f'mae: {mean_absolute_error(y_test, tf_model_preds)}')\n",
        "print(f'mse: {mean_squared_error(y_test, tf_model_preds)}')\n",
        "print(f'rmse: {np.sqrt(mean_squared_error(y_test, tf_model_preds))}')"
      ],
      "metadata": {
        "id": "KsLiDch9Syc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Sequential Model is Not Nearly as Accurate as the RandomForestRegressor and Took Several Hours to Run"
      ],
      "metadata": {
        "id": "h42XvR-TS25S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Recommended Model For This Dataset is a RandomForestRegressor With n_estimators = 130"
      ],
      "metadata": {
        "id": "SaayWkG2S3Kv"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}